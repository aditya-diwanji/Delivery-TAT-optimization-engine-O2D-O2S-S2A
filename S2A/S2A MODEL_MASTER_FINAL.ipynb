{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8c7ca8-1cee-4b71-9bbf-25091f3de2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting the Enhanced TAT Optimization Script with Target Percentages...\n",
      "\n",
      "ğŸ“‹ Configuration Summary:\n",
      "============================================================\n",
      "  ğŸ¯ Target Distribution:\n",
      "    - 1 Day BP: 42.5%\n",
      "    - At least 2 Day BP: 23.0%\n",
      "    - Breach: 7.5%\n",
      "    - On Promise: 27.0%\n",
      "  ğŸ”§ Optimization Weights:\n",
      "    - 1_Day_BP Weight: 1.0\n",
      "    - 2_Day_BP Weight: 0.5\n",
      "    - Breach Penalty: 0.35\n",
      "    - On Promise Penalty: 0.05\n",
      "  âš™ï¸ Fallback Max Breach: 8.5%\n",
      "  ğŸ” Max Iterations: 50, Tolerance: 0.5%\n",
      "\n",
      "ğŸ”„ STEP 1 & 2: LOADING & PREPARING DATA\n",
      "============================================================\n",
      "ğŸ“‚ Loading data from: C:\\Users\\adity\\OneDrive\\Desktop\\TATACLiQ Data\\ModelData\\sample.xlsx\n",
      "âœ… Data loaded successfully!\n",
      "   ğŸ“Š Total records: 17,808\n",
      "   ğŸ“Š Columns found: 27\n",
      "   ğŸ“Š Key columns present: src_clustername, dest_clustername, logisticname, lane, s2a_a, s2d_p, airhdtat\n",
      "\n",
      "ğŸ” Processing delivery status calculations...\n",
      "âœ… Status calculation complete:\n",
      "   ğŸ“Š Records before cleaning: 17,808\n",
      "   ğŸ“Š Records after cleaning: 17,657\n",
      "   ğŸ“Š Records removed: 151\n",
      "\n",
      "ğŸ›£ï¸  Lane Analysis:\n",
      "   ğŸ“Š Unique lanes identified: 9,975\n",
      "ğŸ“ˆ Determining original TAT and baseline metrics for each lane...\n",
      "ğŸ”§ Creating baseline lookup for exact preservation...\n",
      "âœ… Data preparation complete!\n",
      "\n",
      "ğŸ“Š BASELINE Performance Analysis (Before Optimization):\n",
      "============================================================\n",
      "Status               Current      Target       Gap         \n",
      "--------------------------------------------------------\n",
      "1 Day BP             34.31      % 42.50      % +8.19      %\n",
      "At least 2 Day BP    29.48      % 23.00      % -6.48      %\n",
      "Breach               10.87      % 7.50       % -3.37      %\n",
      "On Promise           25.34      % 27.00      % +1.66      %\n",
      "--------------------------------------------------------\n",
      "Total                100.00     % 100.0       +0.00      %\n",
      "\n",
      "ğŸ¤– STEP 3: TRAINING PREDICTION MODEL\n",
      "============================================================\n",
      "ğŸ“Š Preparing training data...\n",
      "   ğŸ“Š Cross-tabulation created with 9975 combinations\n",
      "   ğŸ“Š Training data prepared: 9975 unique lane-TAT combinations\n",
      "ğŸ”§ Encoding categorical variables...\n",
      "   ğŸ“Š Lane types found: ['ROI' 'ZONAL' 'JKNE' 'LOCAL' 'METRO']\n",
      "ğŸš€ Training Random Forest model...\n",
      "   â³ This may take a moment for large datasets...\n",
      "âœ… Model training complete!\n",
      "   ğŸ“Š Model trained on 9975 samples with 2 features\n",
      "\n",
      "ğŸ”® STEP 4: GENERATING & PREDICTING OPTIONS\n",
      "============================================================\n",
      "ğŸ“Š Generating all possible TAT combinations...\n",
      "âœ… Generated 50,552 potential (lane, TAT) combinations\n",
      "   ğŸ“Š Average combinations per lane: 5.1\n",
      "ğŸ”® Making predictions for all combinations...\n",
      "ğŸ”§ Applying baseline preservation for unchanged TAT values...\n",
      "âœ… Baseline preservation applied: 7,455 combinations use exact baseline values\n",
      "ğŸ”§ Normalizing predictions for model-generated combinations...\n",
      "ğŸ“Š Calculating composite scores...\n",
      "âœ… Predictions and scoring complete!\n",
      "\n",
      "ğŸ¯ STEP 5: TARGET-BASED OPTIMIZATION WITH FALLBACK\n",
      "============================================================\n",
      "ğŸš€ Phase 1: Initial optimization using composite scores...\n",
      "âœ… Initial optimization complete for 9975 unique lanes\n",
      "\n",
      "ğŸ¯ Phase 2: Iterative target-based refinement...\n",
      "   ğŸ“Š Attempting to reach target percentages through iterations...\n",
      "   ğŸ”„ Iteration  1: Targeting '1 Day BP' (Error: +26.75%)\n",
      "   ğŸ”„ Iteration  2: Targeting '1 Day BP' (Error: +19.54%)\n",
      "   ğŸ”„ Iteration  3: Targeting '1 Day BP' (Error: +14.23%)\n",
      "   ğŸ”„ Iteration  4: Targeting '1 Day BP' (Error: +10.17%)\n",
      "   ğŸ”„ Iteration  5: Targeting '1 Day BP' (Error: +6.39%)\n",
      "   ğŸ”„ Iteration 10: Targeting 'Breach' (Error: +0.60%)\n",
      "   ğŸ”„ Iteration 15: Targeting '1 Day BP' (Error: +3.35%)\n",
      "   ğŸ”„ Iteration 20: Targeting 'At least 2 Day BP' (Error: +5.35%)\n",
      "   ğŸ”„ Iteration 25: Targeting '1 Day BP' (Error: +1.12%)\n",
      "   ğŸ”„ Iteration 30: Targeting 'On Promise' (Error: +4.69%)\n",
      "   ğŸ”„ Iteration 35: Targeting 'At least 2 Day BP' (Error: -1.69%)\n",
      "   ğŸ”„ Iteration 40: Targeting 'At least 2 Day BP' (Error: +5.37%)\n",
      "   ğŸ”„ Iteration 45: Targeting 'At least 2 Day BP' (Error: -3.83%)\n",
      "   ğŸ”„ Iteration 50: Targeting 'On Promise' (Error: +4.57%)\n",
      "âš ï¸  Reached maximum iterations (50). Optimization stopped.\n",
      "\n",
      "ğŸ›¡ï¸  Phase 3: Fallback Breach Constraint Check...\n",
      "   ğŸ“Š Current optimized breach: 7.35%\n",
      "   ğŸ“Š Fallback breach limit: 8.5%\n",
      "âœ… Breach constraint satisfied! No additional adjustments needed.\n",
      "\n",
      "ğŸ“Š STEP 6 & 7: FINAL ANALYSIS & COMPREHENSIVE REPORTING\n",
      "============================================================\n",
      "ğŸ“ˆ COMPREHENSIVE PERFORMANCE COMPARISON:\n",
      "================================================================================\n",
      "Metric                    Baseline     Optimized    Target     vs Target    vs Baseline \n",
      "-----------------------------------------------------------------------------------\n",
      "1 Day BP                  34.31      % 44.99      % 42.5     % +2.49      % +10.68     %\n",
      "At least 2 Day BP         29.48      % 22.22      % 23.0     % -0.78      % -7.27      %\n",
      "Breach                    10.87      % 7.35       % 7.5      % -0.15      % -3.52      %\n",
      "On Promise                25.34      % 25.45      % 27.0     % -1.55      % +0.11      %\n",
      "-----------------------------------------------------------------------------------\n",
      "Total                     100.00     % 100.00     % 100.0     +0.00      % +0.00      %\n",
      "\n",
      "ğŸ” BASELINE PRESERVATION VERIFICATION:\n",
      "============================================================\n",
      "âœ… Found 2822 lanes with unchanged TAT\n",
      "âœ… VERIFICATION PASSED: All unchanged lanes preserve exact baseline percentages\n",
      "\n",
      "ğŸ”¬ CREATING PARAMETER SENSITIVITY ANALYSIS...\n",
      "============================================================\n",
      "ğŸ“Š Running sensitivity analysis for optimization parameters...\n",
      "   ğŸ“Š Testing 14,641 weight combinations...\n",
      "   ğŸ”„ Processed 0 combinations...\n",
      "   ğŸ”„ Processed 1,000 combinations...\n",
      "   ğŸ”„ Processed 2,000 combinations...\n",
      "   ğŸ”„ Processed 3,000 combinations...\n",
      "   ğŸ”„ Processed 4,000 combinations...\n",
      "   ğŸ”„ Processed 5,000 combinations...\n",
      "   ğŸ”„ Processed 6,000 combinations...\n",
      "   ğŸ”„ Processed 7,000 combinations...\n",
      "   ğŸ”„ Processed 8,000 combinations...\n",
      "   ğŸ”„ Processed 9,000 combinations...\n",
      "   ğŸ”„ Processed 10,000 combinations...\n",
      "   ğŸ”„ Processed 11,000 combinations...\n",
      "   ğŸ”„ Processed 12,000 combinations...\n",
      "   ğŸ”„ Processed 13,000 combinations...\n",
      "   ğŸ”„ Processed 14,000 combinations...\n",
      "ğŸ“ Saving sensitivity table to 'sensitivity_analysis_table.xlsx'...\n",
      "âœ… Sensitivity table saved with 14,641 combinations!\n",
      "\n",
      "ğŸ“‹ TOP 10 WEIGHT COMBINATIONS (Sorted by Breach Proximity):\n",
      "================================================================================\n",
      "1D_BP_W  2D_BP_W  Breach_P OP_P     1 Day BP   2+ Day BP    Breach   On Promise  \n",
      "----------------------------------------------------------------------------\n",
      "0.0      1.5     0.0     12.0    37.81    % 45.67      % 7.49   % 9.02       %\n",
      "0.0      1.5     0.0     13.5    38.25    % 45.24      % 7.55   % 8.97       %\n",
      "0.0      1.5     0.0     10.5    37.22    % 46.24      % 7.44   % 9.10       %\n",
      "0.0      1.5     0.0     15.0    38.64    % 44.83      % 7.61   % 8.93       %\n",
      "0.0      1.5     0.0     9.0     36.36    % 47.08      % 7.33   % 9.23       %\n",
      "0.0      1.5     0.0     7.5     35.68    % 47.81      % 7.15   % 9.36       %\n",
      "0.0      3.0     0.0     15.0    35.68    % 47.81      % 7.15   % 9.36       %\n",
      "1.5      0.0     0.0     15.0    58.53    % 25.29      % 7.14   % 9.03       %\n",
      "0.0      3.0     0.0     13.5    35.16    % 48.33      % 7.04   % 9.47       %\n",
      "1.5      0.0     0.0     13.5    59.26    % 24.63      % 7.00   % 9.11       %\n",
      "\n",
      "ğŸ’¾ SAVING COMPREHENSIVE RESULTS...\n",
      "============================================================\n",
      "ğŸ“ Saving main results to 'enhanced_tat_optimization_results.xlsx'...\n",
      "âœ… Comprehensive results saved!\n",
      "   ğŸ“Š Main optimization results: Sheet 1\n",
      "   ğŸ“Š Parameter sensitivity analysis: Sheet 2\n",
      "   ğŸ“Š Summary metrics comparison: Sheet 3\n",
      "   ğŸ“Š Baseline preservation verification: Sheet 4\n",
      "\n",
      "ğŸ¯ OPTIMIZATION RECOMMENDATIONS & INSIGHTS:\n",
      "============================================================\n",
      "ğŸ† BIGGEST IMPROVEMENT: 1 Day BP (+10.68%)\n",
      "âš ï¸  BIGGEST CONCERN: At least 2 Day BP (-7.27%)\n",
      "\n",
      "ğŸ“Š TAT CHANGE SUMMARY:\n",
      "   ğŸ“Š Lanes with TAT changes: 7,153 out of 9,975 (71.7%)\n",
      "   ğŸ“Š Lanes unchanged: 2,822 (28.3%)\n",
      "   ğŸ“Š Average TAT change: -1.89 days\n",
      "   ğŸ“ˆ TAT increases: 1,246 lanes (avg: +1.43 days)\n",
      "   ğŸ“‰ TAT decreases: 5,907 lanes (avg: -2.59 days)\n",
      "\n",
      "ğŸ¯ TARGET ACHIEVEMENT ANALYSIS:\n",
      "   ğŸŸ¡ 1 Day BP: 44.99% (Target: 42.5%, Gap: +2.49%)\n",
      "   ğŸŸ¡ At least 2 Day BP: 22.22% (Target: 23.0%, Gap: -0.78%)\n",
      "   âœ… Breach: 7.35% (Target: 7.5%, Gap: -0.15%)\n",
      "   ğŸŸ¡ On Promise: 25.45% (Target: 27.0%, Gap: -1.55%)\n",
      "\n",
      "ğŸ’¼ ESTIMATED BUSINESS IMPACT:\n",
      "   ğŸ“Š Total order volume analyzed: 17,657\n",
      "   ğŸ“Š 1 Day BP volume change: +1,886 orders\n",
      "   ğŸ“Š At least 2 Day BP volume change: -1,283 orders\n",
      "   ğŸ“Š Breach volume change: -622 orders\n",
      "   ğŸ“Š On Promise volume change: +19 orders\n",
      "\n",
      "ğŸ”¬ PARAMETER SENSITIVITY INSIGHTS:\n",
      "   ğŸ’¡ Key findings from sensitivity analysis:\n",
      "   ğŸ“Š Most sensitive parameter: score_weight_1day_bp (Breach range: 7.89%)\n",
      "   ğŸ“Š Least sensitive parameter: score_weight_1day_bp (Breach range: 7.89%)\n",
      "\n",
      "ğŸ”’ BASELINE PRESERVATION SUMMARY:\n",
      "   ğŸ“Š Lanes preserving original TAT: 2,822 out of 9,975\n",
      "   âœ… These lanes maintain EXACT baseline percentages\n",
      "   ğŸ“Š Preservation rate: 28.3%\n",
      "\n",
      "ğŸ’¡ ACTIONABLE RECOMMENDATIONS:\n",
      "   1. ğŸ“ˆ Focus on lanes with highest volume impact for manual review\n",
      "   2. ğŸ” Monitor breach percentage closely - current optimized level is balanced\n",
      "   3. âš™ï¸  Consider adjusting parameters based on sensitivity analysis if targets not met\n",
      "   4. ğŸ“Š Implement gradual rollout starting with lanes showing biggest improvements\n",
      "   5. ğŸ”„ Re-run optimization monthly with fresh data for continuous improvement\n",
      "   6. âœ… Validate that unchanged lanes preserve baseline metrics exactly\n",
      "   7. ğŸ¯ Use top sensitivity combinations for fine-tuning toward specific targets\n",
      "\n",
      "ğŸ‰ OPTIMIZATION PROCESS COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "â±ï¸  Total execution time: 627.74 seconds\n",
      "ğŸ“ Main results file: enhanced_tat_optimization_results.xlsx\n",
      "ğŸ“ Sensitivity analysis table: sensitivity_analysis_table.xlsx\n",
      "ğŸ¯ Target achievement status: ğŸŸ¡ Partially Achieved\n",
      "ğŸ”’ Baseline preservation: âœ… Verified\n",
      "\n",
      "âœ¨ Thank you for using the Enhanced TAT Optimization Script! âœ¨\n",
      "ğŸ“§ For questions or improvements, please refer to the comprehensive output files.\n",
      "ğŸ”„ Remember to validate results with business stakeholders before implementation.\n",
      "\n",
      "ğŸ”§ KEY IMPROVEMENTS IN THIS VERSION:\n",
      "   âœ… Exact baseline preservation for unchanged TAT values\n",
      "   âœ… Enhanced verification and reporting\n",
      "   âœ… Improved fallback constraint handling\n",
      "   âœ… Comprehensive baseline preservation verification sheet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"ğŸš€ Starting the Enhanced TAT Optimization Script with Target Percentages...\")\n",
    "\n",
    "# ============ USER CONFIGURATION ============\n",
    "file_path = \"C:\\\\Users\\\\adity\\\\OneDrive\\\\Desktop\\\\TATACLiQ Data\\\\ModelData\\\\sample.xlsx\" #<-- Use your full data file\n",
    "\n",
    "# --- STRATEGIC TARGETS ---\n",
    "target_percentages = {\n",
    "    '1 Day BP': 42.5,\n",
    "    'At least 2 Day BP': 23.0,\n",
    "    'Breach': 7.5,\n",
    "    'On Promise': 27.0\n",
    "}\n",
    "\n",
    "max_breach = 8.5          # Fallback constraint if target approach doesn't work\n",
    "\n",
    "# --- STRATEGIC TUNING PARAMETERS ---\n",
    "score_weight_1day_bp = 1.0\n",
    "score_weight_2day_bp = 0.5\n",
    "score_penalty_breach = 0.35\n",
    "score_penalty_on_promise = 0.05\n",
    "\n",
    "# Additional parameters for target-based optimization\n",
    "max_iterations = 50\n",
    "tolerance = 0.5\n",
    "\n",
    "# Lane type TAT upper bounds\n",
    "lane_tat_limits = {\n",
    "    'METRO': 4,\n",
    "    'LOCAL': 2,\n",
    "    'ZONAL': 5,\n",
    "    'ROI': 6,\n",
    "    'JKNE': 7\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Configuration Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  ğŸ¯ Target Distribution:\")\n",
    "for status, target in target_percentages.items():\n",
    "    print(f\"    - {status}: {target}%\")\n",
    "print(f\"  ğŸ”§ Optimization Weights:\")\n",
    "print(f\"    - 1_Day_BP Weight: {score_weight_1day_bp}\")\n",
    "print(f\"    - 2_Day_BP Weight: {score_weight_2day_bp}\")\n",
    "print(f\"    - Breach Penalty: {score_penalty_breach}\")\n",
    "print(f\"    - On Promise Penalty: {score_penalty_on_promise}\")\n",
    "print(f\"  âš™ï¸ Fallback Max Breach: {max_breach}%\")\n",
    "print(f\"  ğŸ” Max Iterations: {max_iterations}, Tolerance: {tolerance}%\")\n",
    "\n",
    "# ============ LOAD & PREPARE DATA (STEP 1 & 2) ============\n",
    "print(f\"\\nğŸ”„ STEP 1 & 2: LOADING & PREPARING DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“‚ Loading data from: {file_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"âœ… Data loaded successfully!\")\n",
    "    print(f\"   ğŸ“Š Total records: {len(df):,}\")\n",
    "    print(f\"   ğŸ“Š Columns found: {len(df.columns)}\")\n",
    "    print(f\"   ğŸ“Š Key columns present: {', '.join([col for col in ['src_clustername', 'dest_clustername', 'logisticname', 'lane', 's2a_a', 's2d_p', 'airhdtat'] if col in df.columns])}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: File not found at '{file_path}'. Please check the path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR loading file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# CALCULATE s2a_status\n",
    "print(\"\\nğŸ” Processing delivery status calculations...\")\n",
    "def calculate_s2a_status(row):\n",
    "    s2a_a = row['s2a_a']\n",
    "    s2d_p = row['s2d_p'] if 's2d_p' in row else row['airhdtat']\n",
    "    if pd.isna(s2a_a) or pd.isna(s2d_p): return None\n",
    "    if s2a_a == s2d_p: return 'On Promise'\n",
    "    elif s2a_a > s2d_p: return 'Breach'\n",
    "    elif s2d_p - s2a_a == 1: return '1 Day BP'\n",
    "    else: return 'At least 2 Day BP'\n",
    "\n",
    "# Handle missing s2d_p column\n",
    "if 's2d_p' not in df.columns:\n",
    "    print(\"âš ï¸  's2d_p' column not found, using 'airhdtat' as promised delivery days.\")\n",
    "    df['s2d_p'] = df['airhdtat']\n",
    "\n",
    "df['s2a_status'] = df.apply(calculate_s2a_status, axis=1)\n",
    "records_before_cleaning = len(df)\n",
    "df = df.dropna(subset=['s2a_status'])\n",
    "records_after_cleaning = len(df)\n",
    "print(f\"âœ… Status calculation complete:\")\n",
    "print(f\"   ğŸ“Š Records before cleaning: {records_before_cleaning:,}\")\n",
    "print(f\"   ğŸ“Š Records after cleaning: {records_after_cleaning:,}\")\n",
    "print(f\"   ğŸ“Š Records removed: {records_before_cleaning - records_after_cleaning:,}\")\n",
    "\n",
    "# CREATE lane_id and capture original metrics\n",
    "status_categories = ['1 Day BP', 'At least 2 Day BP', 'Breach', 'On Promise']\n",
    "df['lane_id'] = (df['src_clustername'].astype(str) + '->' + \n",
    "                 df['dest_clustername'].astype(str) + '->' + \n",
    "                 df['logisticname'].astype(str) + '->' + \n",
    "                 df['lane'].astype(str))\n",
    "\n",
    "print(f\"\\nğŸ›£ï¸  Lane Analysis:\")\n",
    "unique_lanes = df['lane_id'].nunique()\n",
    "print(f\"   ğŸ“Š Unique lanes identified: {unique_lanes:,}\")\n",
    "\n",
    "# Capture original TAT and baseline metrics per lane\n",
    "print(\"ğŸ“ˆ Determining original TAT and baseline metrics for each lane...\")\n",
    "original_tats = df.groupby('lane_id')['airhdtat'].agg(lambda x: x.mode().iloc[0]).reset_index()\n",
    "original_tats.rename(columns={'airhdtat': 'original_airhdtat'}, inplace=True)\n",
    "\n",
    "baseline_metrics = []\n",
    "for lane_id in df['lane_id'].unique():\n",
    "    lane_data = df[df['lane_id'] == lane_id]\n",
    "    total_volume = len(lane_data)\n",
    "    \n",
    "    lane_metrics = {\n",
    "        'lane_id': lane_id,\n",
    "        'volume': total_volume,\n",
    "        'baseline_1_Day_BP%': (lane_data['s2a_status'] == '1 Day BP').sum() / total_volume * 100,\n",
    "        'baseline_At_least_2_Day_BP%': (lane_data['s2a_status'] == 'At least 2 Day BP').sum() / total_volume * 100,\n",
    "        'baseline_Breach%': (lane_data['s2a_status'] == 'Breach').sum() / total_volume * 100,\n",
    "        'baseline_On_Promise%': (lane_data['s2a_status'] == 'On Promise').sum() / total_volume * 100\n",
    "    }\n",
    "    baseline_metrics.append(lane_metrics)\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_metrics)\n",
    "\n",
    "# CREATE BASELINE LOOKUP DICTIONARY for exact preservation\n",
    "print(\"ğŸ”§ Creating baseline lookup for exact preservation...\")\n",
    "baseline_lookup = {}\n",
    "for _, row in baseline_df.iterrows():\n",
    "    original_tat = original_tats[original_tats['lane_id'] == row['lane_id']]['original_airhdtat'].iloc[0]\n",
    "    baseline_lookup[(row['lane_id'], original_tat)] = {\n",
    "        '1 Day BP%': row['baseline_1_Day_BP%'],\n",
    "        'At least 2 Day BP%': row['baseline_At_least_2_Day_BP%'],\n",
    "        'Breach%': row['baseline_Breach%'],\n",
    "        'On Promise%': row['baseline_On_Promise%']\n",
    "    }\n",
    "\n",
    "print(\"âœ… Data preparation complete!\")\n",
    "\n",
    "# ============ BASELINE METRICS ============\n",
    "print(f\"\\nğŸ“Š BASELINE Performance Analysis (Before Optimization):\")\n",
    "print(\"=\" * 60)\n",
    "total_records = len(df)\n",
    "baseline_percentages = {status: (df['s2a_status'] == status).sum() / total_records * 100 for status in status_categories}\n",
    "\n",
    "print(f\"{'Status':<20} {'Current':<12} {'Target':<12} {'Gap':<12}\")\n",
    "print(\"-\" * 56)\n",
    "for status, percentage in baseline_percentages.items():\n",
    "    target = target_percentages[status]\n",
    "    gap = target - percentage\n",
    "    print(f\"{status:<20} {percentage:<11.2f}% {target:<11.2f}% {gap:<+11.2f}%\")\n",
    "print(\"-\" * 56)\n",
    "total_baseline = sum(baseline_percentages.values())\n",
    "print(f\"{'Total':<20} {total_baseline:<11.2f}% {'100.0':<11} {100.0 - total_baseline:<+11.2f}%\")\n",
    "\n",
    "# ============ TRAIN MODEL (STEP 3) ============\n",
    "print(f\"\\nğŸ¤– STEP 3: TRAINING PREDICTION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š Preparing training data...\")\n",
    "\n",
    "grouped = pd.crosstab([df['lane_id'], df['airhdtat']], df['s2a_status'])\n",
    "print(f\"   ğŸ“Š Cross-tabulation created with {len(grouped)} combinations\")\n",
    "\n",
    "for status in status_categories:\n",
    "    if status not in grouped.columns: \n",
    "        grouped[status] = 0\n",
    "        print(f\"   âš ï¸  Added missing status category: {status}\")\n",
    "\n",
    "grouped['total'] = grouped[status_categories].sum(axis=1)\n",
    "for status in status_categories:\n",
    "    grouped[f'{status}%'] = 100 * grouped[status] / grouped['total']\n",
    "\n",
    "final_df = grouped.reset_index()\n",
    "final_df = final_df[final_df['total'] > 0]\n",
    "print(f\"   ğŸ“Š Training data prepared: {len(final_df)} unique lane-TAT combinations\")\n",
    "\n",
    "print(\"ğŸ”§ Encoding categorical variables...\")\n",
    "le = LabelEncoder()\n",
    "final_df['lane_encoded'] = le.fit_transform(final_df['lane_id'])\n",
    "final_df['lane_type'] = final_df['lane_id'].str.split('->').str[-1]\n",
    "print(f\"   ğŸ“Š Lane types found: {final_df['lane_type'].unique()}\")\n",
    "\n",
    "X = final_df[['lane_encoded', 'airhdtat']]\n",
    "y = final_df[[f'{status}%' for status in status_categories]]\n",
    "\n",
    "print(\"ğŸš€ Training Random Forest model...\")\n",
    "print(\"   â³ This may take a moment for large datasets...\")\n",
    "model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "model.fit(X, y)\n",
    "print(\"âœ… Model training complete!\")\n",
    "print(f\"   ğŸ“Š Model trained on {len(X)} samples with {X.shape[1]} features\")\n",
    "\n",
    "# ============ GENERATE & PREDICT OPTIONS (STEP 4) ============\n",
    "print(f\"\\nğŸ”® STEP 4: GENERATING & PREDICTING OPTIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š Generating all possible TAT combinations...\")\n",
    "\n",
    "unique_lanes_df = final_df.drop_duplicates(subset='lane_id').copy()\n",
    "tat_options = []\n",
    "total_combinations = 0\n",
    "\n",
    "for idx, row in unique_lanes_df.iterrows():\n",
    "    lane_id, lane_encoded, current_tat, lane_type = row['lane_id'], row['lane_encoded'], int(row['airhdtat']), row['lane_type']\n",
    "    total_volume = final_df[final_df['lane_id'] == lane_id]['total'].sum()\n",
    "    max_tat = lane_tat_limits.get(lane_type, current_tat + 2)\n",
    "    \n",
    "    lane_combinations = 0\n",
    "    for tat in range(1, max_tat + 1):\n",
    "        tat_options.append({\n",
    "            'lane_id': lane_id, \n",
    "            'lane_encoded': lane_encoded, \n",
    "            'original_tat': current_tat, \n",
    "            'total': total_volume, \n",
    "            'tat': tat, \n",
    "            'lane_type': lane_type\n",
    "        })\n",
    "        lane_combinations += 1\n",
    "    total_combinations += lane_combinations\n",
    "\n",
    "tat_df = pd.DataFrame(tat_options)\n",
    "print(f\"âœ… Generated {len(tat_df):,} potential (lane, TAT) combinations\")\n",
    "print(f\"   ğŸ“Š Average combinations per lane: {len(tat_df) / len(unique_lanes_df):.1f}\")\n",
    "\n",
    "print(\"ğŸ”® Making predictions for all combinations...\")\n",
    "X_all = tat_df[['lane_encoded', 'tat']].rename(columns={'tat': 'airhdtat'})\n",
    "y_all = model.predict(X_all)\n",
    "\n",
    "for i, status in enumerate(status_categories):\n",
    "    tat_df[f'{status}%'] = y_all[:, i]\n",
    "\n",
    "# *** CRITICAL FIX: Override predictions with exact baseline values when TAT is unchanged ***\n",
    "print(\"ğŸ”§ Applying baseline preservation for unchanged TAT values...\")\n",
    "baseline_preserved_count = 0\n",
    "for idx, row in tat_df.iterrows():\n",
    "    lookup_key = (row['lane_id'], row['tat'])\n",
    "    if lookup_key in baseline_lookup:\n",
    "        # Use exact baseline values when TAT matches original\n",
    "        baseline_values = baseline_lookup[lookup_key]\n",
    "        for status in status_categories:\n",
    "            tat_df.at[idx, f'{status}%'] = baseline_values[f'{status}%']\n",
    "        baseline_preserved_count += 1\n",
    "\n",
    "print(f\"âœ… Baseline preservation applied: {baseline_preserved_count:,} combinations use exact baseline values\")\n",
    "\n",
    "# Normalize predictions to ensure they sum to 100% (only for non-baseline combinations)\n",
    "print(\"ğŸ”§ Normalizing predictions for model-generated combinations...\")\n",
    "for idx, row in tat_df.iterrows():\n",
    "    lookup_key = (row['lane_id'], row['tat'])\n",
    "    if lookup_key not in baseline_lookup:  # Only normalize non-baseline predictions\n",
    "        percentage_cols = [f'{status}%' for status in status_categories]\n",
    "        row_sum = sum(tat_df.loc[idx, col] for col in percentage_cols)\n",
    "        if row_sum > 0:\n",
    "            for col in percentage_cols:\n",
    "                tat_df.at[idx, col] = (tat_df.at[idx, col] / row_sum) * 100\n",
    "\n",
    "print(\"ğŸ“Š Calculating composite scores...\")\n",
    "tat_df['composite_score'] = (\n",
    "    (score_weight_1day_bp * tat_df['1 Day BP%']) + \n",
    "    (score_weight_2day_bp * tat_df['At least 2 Day BP%']) - \n",
    "    (score_penalty_breach * tat_df['Breach%']) - \n",
    "    (score_penalty_on_promise * tat_df['On Promise%'])\n",
    ")\n",
    "print(\"âœ… Predictions and scoring complete!\")\n",
    "\n",
    "# ============ TARGET-BASED OPTIMIZATION (STEP 5) ============\n",
    "print(f\"\\nğŸ¯ STEP 5: TARGET-BASED OPTIMIZATION WITH FALLBACK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start with best composite score selections\n",
    "print(\"ğŸš€ Phase 1: Initial optimization using composite scores...\")\n",
    "best_tat_idx = tat_df.groupby('lane_id')['composite_score'].idxmax()\n",
    "opt_df = tat_df.loc[best_tat_idx].copy().reset_index(drop=True)\n",
    "\n",
    "# Ensure opt_df has unique lane_ids by removing any duplicates\n",
    "if opt_df['lane_id'].duplicated().any():\n",
    "    print(\"   âš ï¸  Removing duplicate lane_ids from initial selection...\")\n",
    "    opt_df = opt_df.drop_duplicates(subset='lane_id', keep='first').reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Initial optimization complete for {len(opt_df)} unique lanes\")\n",
    "\n",
    "def calculate_weighted_percentages(df):\n",
    "    results = {}\n",
    "    total_volume = df['total'].sum()\n",
    "    for status in status_categories:\n",
    "        results[status] = (df[f'{status}%'] * df['total']).sum() / total_volume\n",
    "    return results\n",
    "\n",
    "print(\"\\nğŸ¯ Phase 2: Iterative target-based refinement...\")\n",
    "print(\"   ğŸ“Š Attempting to reach target percentages through iterations...\")\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    current_percentages = calculate_weighted_percentages(opt_df)\n",
    "    errors = {s: current_percentages[s] - target_percentages[s] for s in status_categories}\n",
    "    \n",
    "    # Check if targets are achieved\n",
    "    max_error = max(abs(e) for e in errors.values())\n",
    "    if max_error < tolerance:\n",
    "        print(f\"âœ… Targets achieved within tolerance after {i} iterations!\")\n",
    "        print(f\"   ğŸ“Š Maximum error: {max_error:.2f}%\")\n",
    "        break\n",
    "    \n",
    "    # Find the metric that needs the most adjustment\n",
    "    metric_to_fix = max(errors, key=lambda k: abs(errors[k]))\n",
    "    direction = -1 if errors[metric_to_fix] > 0 else 1\n",
    "    \n",
    "    if (i + 1) % 5 == 0 or i < 5:\n",
    "        print(f\"   ğŸ”„ Iteration {i+1:2d}: Targeting '{metric_to_fix}' (Error: {errors[metric_to_fix]:+.2f}%)\")\n",
    "    \n",
    "    # Find lanes that can improve the target metric\n",
    "    current_choices = opt_df[['lane_id', 'tat', f'{metric_to_fix}%', 'total']].copy()\n",
    "    alternatives = tat_df[['lane_id', 'tat', f'{metric_to_fix}%', 'total']].copy()\n",
    "    \n",
    "    merged = pd.merge(current_choices, alternatives, on='lane_id', suffixes=('_cur', '_alt'))\n",
    "    mask = merged['tat_cur'] != merged['tat_alt']\n",
    "    merged = merged[mask]\n",
    "    \n",
    "    merged['improvement'] = (merged[f'{metric_to_fix}%_alt'] - merged[f'{metric_to_fix}%_cur']) * direction\n",
    "    merged = merged[merged['improvement'] > 0]\n",
    "    \n",
    "    if merged.empty:\n",
    "        print(f\"âš ï¸  No further improvement possible for '{metric_to_fix}'. Stopping at iteration {i+1}\")\n",
    "        break\n",
    "    \n",
    "    merged['impact'] = merged['improvement'] * merged['total_alt']\n",
    "    merged = merged.sort_values('impact', ascending=False)\n",
    "    \n",
    "    # Update top lanes\n",
    "    n_update = max(1, int(0.05 * len(opt_df)))\n",
    "    updates = merged.head(n_update)\n",
    "    \n",
    "    for _, row in updates.iterrows():\n",
    "        matching_rows = tat_df[(tat_df['lane_id'] == row['lane_id']) & (tat_df['tat'] == row['tat_alt'])]\n",
    "        if not matching_rows.empty:\n",
    "            lane_mask = opt_df['lane_id'] == row['lane_id']\n",
    "            if lane_mask.any():\n",
    "                new_row_data = matching_rows.iloc[0]\n",
    "                for col in new_row_data.index:\n",
    "                    if col in opt_df.columns:\n",
    "                        opt_df.loc[lane_mask, col] = new_row_data[col]\n",
    "\n",
    "else:\n",
    "    print(f\"âš ï¸  Reached maximum iterations ({max_iterations}). Optimization stopped.\")\n",
    "\n",
    "# ============ FALLBACK CONSTRAINT CHECK ============\n",
    "print(f\"\\nğŸ›¡ï¸  Phase 3: Fallback Breach Constraint Check...\")\n",
    "def weighted_breach(df): \n",
    "    return (df['Breach%'] * df['total']).sum() / df['total'].sum()\n",
    "\n",
    "current_breach = weighted_breach(opt_df)\n",
    "print(f\"   ğŸ“Š Current optimized breach: {current_breach:.2f}%\")\n",
    "print(f\"   ğŸ“Š Fallback breach limit: {max_breach}%\")\n",
    "\n",
    "if current_breach <= max_breach:\n",
    "    print(\"âœ… Breach constraint satisfied! No additional adjustments needed.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Breach constraint not met. Applying fallback reversion process...\")\n",
    "    \n",
    "    # Create lookup for reverting to exact baseline values\n",
    "    final_df_baseline_lookup = {}\n",
    "    for _, row in final_df.iterrows():\n",
    "        key = (row['lane_id'], row['airhdtat'])\n",
    "        final_df_baseline_lookup[key] = {\n",
    "            'Breach%': row['Breach%'],\n",
    "            '1 Day BP%': row['1 Day BP%'],\n",
    "            'At least 2 Day BP%': row['At least 2 Day BP%'],\n",
    "            'On Promise%': row['On Promise%']\n",
    "        }\n",
    "    \n",
    "    def get_original_breach(row):\n",
    "        lookup_key = (row['lane_id'], row['original_tat'])\n",
    "        if lookup_key in final_df_baseline_lookup:\n",
    "            return final_df_baseline_lookup[lookup_key]['Breach%']\n",
    "        elif lookup_key in baseline_lookup:\n",
    "            return baseline_lookup[lookup_key]['Breach%']\n",
    "        else:\n",
    "            return row['Breach%']\n",
    "    \n",
    "    opt_df['original_breach'] = opt_df.apply(get_original_breach, axis=1)\n",
    "    opt_df['breach_increase'] = opt_df['Breach%'] - opt_df['original_breach']\n",
    "    \n",
    "    # Stage 1: Revert worst offenders\n",
    "    print(\"   ğŸ”„ Stage 1: Reverting lanes with highest breach increase...\")\n",
    "    revert_candidates = opt_df[(opt_df['tat'] != opt_df['original_tat']) & (opt_df['breach_increase'] > 0)]\n",
    "    revert_candidates = revert_candidates.sort_values('breach_increase', ascending=False)\n",
    "    \n",
    "    reverted_count = 0\n",
    "    for idx, row in revert_candidates.iterrows():\n",
    "        if weighted_breach(opt_df) <= max_breach: \n",
    "            break\n",
    "        \n",
    "        # Revert to exact baseline values\n",
    "        baseline_key = (row['lane_id'], row['original_tat'])\n",
    "        if baseline_key in baseline_lookup:\n",
    "            opt_df.loc[idx, 'tat'] = row['original_tat']\n",
    "            baseline_values = baseline_lookup[baseline_key]\n",
    "            for status in status_categories:\n",
    "                opt_df.loc[idx, f'{status}%'] = baseline_values[f'{status}%']\n",
    "            reverted_count += 1\n",
    "        elif baseline_key in final_df_baseline_lookup:\n",
    "            opt_df.loc[idx, 'tat'] = row['original_tat']\n",
    "            baseline_values = final_df_baseline_lookup[baseline_key]\n",
    "            for status in status_categories:\n",
    "                opt_df.loc[idx, f'{status}%'] = baseline_values[f'{status}%']\n",
    "            reverted_count += 1\n",
    "    \n",
    "    print(f\"      ğŸ“Š Reverted {reverted_count} lanes in Stage 1\")\n",
    "    print(f\"      ğŸ“Š Breach after Stage 1: {weighted_breach(opt_df):.2f}%\")\n",
    "    \n",
    "    # Stage 2: If still over, revert by volume\n",
    "    if weighted_breach(opt_df) > max_breach:\n",
    "        print(\"   ğŸ”„ Stage 2: Reverting high-volume lanes...\")\n",
    "        remaining_candidates = opt_df[opt_df['tat'] != opt_df['original_tat']].sort_values('total', ascending=False)\n",
    "        stage2_reverts = 0\n",
    "        for idx, row in remaining_candidates.iterrows():\n",
    "            if weighted_breach(opt_df) <= max_breach: \n",
    "                break\n",
    "            \n",
    "            # Revert to exact baseline values\n",
    "            baseline_key = (row['lane_id'], row['original_tat'])\n",
    "            if baseline_key in baseline_lookup:\n",
    "                opt_df.loc[idx, 'tat'] = row['original_tat']\n",
    "                baseline_values = baseline_lookup[baseline_key]\n",
    "                for status in status_categories:\n",
    "                    opt_df.loc[idx, f'{status}%'] = baseline_values[f'{status}%']\n",
    "                stage2_reverts += 1\n",
    "            elif baseline_key in final_df_baseline_lookup:\n",
    "                opt_df.loc[idx, 'tat'] = row['original_tat']\n",
    "                baseline_values = final_df_baseline_lookup[baseline_key]\n",
    "                for status in status_categories:\n",
    "                    opt_df.loc[idx, f'{status}%'] = baseline_values[f'{status}%']\n",
    "                stage2_reverts += 1\n",
    "        print(f\"      ğŸ“Š Reverted additional {stage2_reverts} lanes in Stage 2\")\n",
    "    \n",
    "    final_breach = weighted_breach(opt_df)\n",
    "    print(f\"âœ… Reversion complete. Final breach: {final_breach:.2f}%\")\n",
    "\n",
    "# ============ FINAL ANALYSIS & SAVE (STEP 6 & 7) ============\n",
    "print(f\"\\nğŸ“Š STEP 6 & 7: FINAL ANALYSIS & COMPREHENSIVE REPORTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Merge comprehensive data\n",
    "opt_df = pd.merge(opt_df, original_tats, on='lane_id', how='left')\n",
    "opt_df = pd.merge(opt_df, baseline_df, on='lane_id', how='left')\n",
    "\n",
    "# Add lane components\n",
    "opt_df[['src_clustername', 'dest_clustername', 'logisticname', 'lane']] = opt_df['lane_id'].str.split('->', expand=True)\n",
    "\n",
    "print(\"ğŸ“ˆ COMPREHENSIVE PERFORMANCE COMPARISON:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<25} {'Baseline':<12} {'Optimized':<12} {'Target':<10} {'vs Target':<12} {'vs Baseline':<12}\")\n",
    "print(\"-\" * 83)\n",
    "\n",
    "# Calculate final weighted percentages\n",
    "final_percentages = calculate_weighted_percentages(opt_df)\n",
    "\n",
    "for status in status_categories:\n",
    "    baseline_val = baseline_percentages[status]\n",
    "    optimized_val = final_percentages[status]\n",
    "    target_val = target_percentages[status]\n",
    "    vs_target = optimized_val - target_val\n",
    "    vs_baseline = optimized_val - baseline_val\n",
    "    \n",
    "    print(f\"{status:<25} {baseline_val:<11.2f}% {optimized_val:<11.2f}% {target_val:<9.1f}% {vs_target:<+11.2f}% {vs_baseline:<+11.2f}%\")\n",
    "\n",
    "print(\"-\" * 83)\n",
    "total_optimized = sum(final_percentages.values())\n",
    "print(f\"{'Total':<25} {sum(baseline_percentages.values()):<11.2f}% {total_optimized:<11.2f}% {'100.0':<9} {total_optimized-100:<+11.2f}% {total_optimized-sum(baseline_percentages.values()):<+11.2f}%\")\n",
    "\n",
    "# ============ VERIFICATION OF BASELINE PRESERVATION ============\n",
    "print(f\"\\nğŸ” BASELINE PRESERVATION VERIFICATION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "unchanged_lanes = opt_df[opt_df['tat'] == opt_df['original_airhdtat']]\n",
    "if len(unchanged_lanes) > 0:\n",
    "    print(f\"âœ… Found {len(unchanged_lanes)} lanes with unchanged TAT\")\n",
    "    \n",
    "    # Verify that baseline percentages are exactly preserved\n",
    "    verification_passed = True\n",
    "    total_errors = 0\n",
    "    \n",
    "    for idx, row in unchanged_lanes.iterrows():\n",
    "        lane_id = row['lane_id']\n",
    "        original_tat = row['original_airhdtat']\n",
    "        \n",
    "        for status in status_categories:\n",
    "            optimized_pct = row[f'{status}%']\n",
    "            baseline_pct = row[f'baseline_{status.replace(\" \", \"_\")}%']\n",
    "            \n",
    "            error = abs(optimized_pct - baseline_pct)\n",
    "            if error > 0.001:  # Allow for tiny floating point errors\n",
    "                verification_passed = False\n",
    "                total_errors += 1\n",
    "    \n",
    "    if verification_passed:\n",
    "        print(\"âœ… VERIFICATION PASSED: All unchanged lanes preserve exact baseline percentages\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  VERIFICATION WARNING: {total_errors} percentage mismatches found in unchanged lanes\")\n",
    "        print(\"   This may indicate model prediction errors that were not properly corrected\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  No lanes kept their original TAT values\")\n",
    "\n",
    "# ============ PARAMETER SENSITIVITY ANALYSIS ============\n",
    "print(f\"\\nğŸ”¬ CREATING PARAMETER SENSITIVITY ANALYSIS...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_sensitivity_analysis():\n",
    "    print(\"ğŸ“Š Running sensitivity analysis for optimization parameters...\")\n",
    "    \n",
    "    # Define parameter ranges for testing (0 to 15)\n",
    "    param_ranges = {\n",
    "        'score_weight_1day_bp': np.arange(0, 15.1, 1.5),  # Step 1.5 for ~10 values\n",
    "        'score_weight_2day_bp': np.arange(0, 15.1, 1.5),\n",
    "        'score_penalty_breach': np.arange(0, 15.1, 1.5),\n",
    "        'score_penalty_on_promise': np.arange(0, 15.1, 1.5)\n",
    "    }\n",
    "    \n",
    "    base_params = {\n",
    "        'score_weight_1day_bp': score_weight_1day_bp,\n",
    "        'score_weight_2day_bp': score_weight_2day_bp,\n",
    "        'score_penalty_breach': score_penalty_breach,\n",
    "        'score_penalty_on_promise': score_penalty_on_promise\n",
    "    }\n",
    "    \n",
    "    sensitivity_results = []\n",
    "    \n",
    "    # Generate combinations\n",
    "    from itertools import product\n",
    "    param_combinations = list(product(\n",
    "        param_ranges['score_weight_1day_bp'],\n",
    "        param_ranges['score_weight_2day_bp'],\n",
    "        param_ranges['score_penalty_breach'],\n",
    "        param_ranges['score_penalty_on_promise']\n",
    "    ))\n",
    "    \n",
    "    print(f\"   ğŸ“Š Testing {len(param_combinations):,} weight combinations...\")\n",
    "    \n",
    "    for idx, (w1, w2, p_b, p_op) in enumerate(param_combinations):\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"   ğŸ”„ Processed {idx:,} combinations...\")\n",
    "        \n",
    "        # Recalculate composite scores\n",
    "        test_tat_df = tat_df.copy()\n",
    "        test_tat_df['composite_score'] = (\n",
    "            (w1 * test_tat_df['1 Day BP%']) + \n",
    "            (w2 * test_tat_df['At least 2 Day BP%']) - \n",
    "            (p_b * test_tat_df['Breach%']) - \n",
    "            (p_op * test_tat_df['On Promise%'])\n",
    "        )\n",
    "        \n",
    "        # Get best selections\n",
    "        test_best_idx = test_tat_df.groupby('lane_id')['composite_score'].idxmax()\n",
    "        test_opt_df = test_tat_df.loc[test_best_idx].copy()\n",
    "        \n",
    "        # Calculate results\n",
    "        test_results = calculate_weighted_percentages(test_opt_df)\n",
    "        \n",
    "        # Store results\n",
    "        result_row = {\n",
    "            'score_weight_1day_bp': w1,\n",
    "            'score_weight_2day_bp': w2,\n",
    "            'score_penalty_breach': p_b,\n",
    "            'score_penalty_on_promise': p_op,\n",
    "            'is_baseline': (w1 == base_params['score_weight_1day_bp'] and\n",
    "                           w2 == base_params['score_weight_2day_bp'] and\n",
    "                           p_b == base_params['score_penalty_breach'] and\n",
    "                           p_op == base_params['score_penalty_on_promise']),\n",
    "            '1 Day BP': test_results['1 Day BP'],\n",
    "            'At least 2 Day BP': test_results['At least 2 Day BP'],\n",
    "            'Breach': test_results['Breach'],\n",
    "            'On Promise': test_results['On Promise'],\n",
    "            'breach_error': abs(test_results['Breach'] - target_percentages['Breach'])\n",
    "        }\n",
    "        sensitivity_results.append(result_row)\n",
    "    \n",
    "    sensitivity_df = pd.DataFrame(sensitivity_results)\n",
    "    \n",
    "    # Sort by proximity to target breach (7.5%)\n",
    "    sensitivity_df = sensitivity_df.sort_values('breach_error').reset_index(drop=True)\n",
    "    \n",
    "    return sensitivity_df\n",
    "\n",
    "sensitivity_df = run_sensitivity_analysis()\n",
    "\n",
    "# Save sensitivity table\n",
    "sensitivity_table_filename = \"sensitivity_analysis_table.xlsx\"\n",
    "print(f\"ğŸ“ Saving sensitivity table to '{sensitivity_table_filename}'...\")\n",
    "sensitivity_df.to_excel(sensitivity_table_filename, index=False)\n",
    "print(f\"âœ… Sensitivity table saved with {len(sensitivity_df):,} combinations!\")\n",
    "\n",
    "# Print top 10 combinations closest to Breach = 7.5%\n",
    "print(\"\\nğŸ“‹ TOP 10 WEIGHT COMBINATIONS (Sorted by Breach Proximity):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'1D_BP_W':<8} {'2D_BP_W':<8} {'Breach_P':<8} {'OP_P':<8} {'1 Day BP':<10} {'2+ Day BP':<12} {'Breach':<8} {'On Promise':<12}\")\n",
    "print(\"-\" * 76)\n",
    "for _, row in sensitivity_df.head(10).iterrows():\n",
    "    marker = \"*\" if row['is_baseline'] else \" \"\n",
    "    print(f\"{row['score_weight_1day_bp']:<7.1f}{marker} {row['score_weight_2day_bp']:<7.1f} {row['score_penalty_breach']:<7.1f} {row['score_penalty_on_promise']:<7.1f} \"\n",
    "          f\"{row['1 Day BP']:<9.2f}% {row['At least 2 Day BP']:<11.2f}% {row['Breach']:<7.2f}% {row['On Promise']:<11.2f}%\")\n",
    "\n",
    "# ============ SAVE COMPREHENSIVE RESULTS ============\n",
    "print(f\"\\nğŸ’¾ SAVING COMPREHENSIVE RESULTS...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare final output with all relevant columns\n",
    "output_columns = [\n",
    "    'lane_id', 'src_clustername', 'dest_clustername', 'logisticname', 'lane',\n",
    "    'volume', 'original_airhdtat', 'tat',\n",
    "    'baseline_1_Day_BP%', '1 Day BP%',\n",
    "    'baseline_At_least_2_Day_BP%', 'At least 2 Day BP%', \n",
    "    'baseline_Breach%', 'Breach%',\n",
    "    'baseline_On_Promise%', 'On Promise%'\n",
    "]\n",
    "\n",
    "# Rename optimized columns for clarity\n",
    "opt_df_final = opt_df.copy()\n",
    "opt_df_final.rename(columns={\n",
    "    'tat': 'optimized_airhdtat',\n",
    "    '1 Day BP%': 'optimized_1_Day_BP%',\n",
    "    'At least 2 Day BP%': 'optimized_At_least_2_Day_BP%',\n",
    "    'Breach%': 'optimized_Breach%',\n",
    "    'On Promise%': 'optimized_On_Promise%'\n",
    "}, inplace=True)\n",
    "\n",
    "# Update column list with renamed columns\n",
    "final_output_columns = [\n",
    "    'lane_id', 'src_clustername', 'dest_clustername', 'logisticname', 'lane',\n",
    "    'volume', 'original_airhdtat', 'optimized_airhdtat',\n",
    "    'baseline_1_Day_BP%', 'optimized_1_Day_BP%',\n",
    "    'baseline_At_least_2_Day_BP%', 'optimized_At_least_2_Day_BP%', \n",
    "    'baseline_Breach%', 'optimized_Breach%',\n",
    "    'baseline_On_Promise%', 'optimized_On_Promise%'\n",
    "]\n",
    "\n",
    "output_filename = \"enhanced_tat_optimization_results.xlsx\"\n",
    "print(f\"ğŸ“ Saving main results to '{output_filename}'...\")\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "    # Main results\n",
    "    opt_df_final[final_output_columns].to_excel(writer, sheet_name='Optimization_Results', index=False)\n",
    "    \n",
    "    # Sensitivity analysis\n",
    "    sensitivity_df.to_excel(writer, sheet_name='Sensitivity_Analysis', index=False)\n",
    "    \n",
    "    # Summary metrics\n",
    "    summary_data = []\n",
    "    for status in status_categories:\n",
    "        summary_data.append({\n",
    "            'Metric': status,\n",
    "            'Baseline_%': baseline_percentages[status],\n",
    "            'Optimized_%': final_percentages[status],\n",
    "            'Target_%': target_percentages[status],\n",
    "            'Change_vs_Baseline': final_percentages[status] - baseline_percentages[status],\n",
    "            'Gap_to_Target': final_percentages[status] - target_percentages[status]\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary_Metrics', index=False)\n",
    "    \n",
    "    # Baseline preservation verification sheet\n",
    "    verification_data = []\n",
    "    unchanged_lanes_data = opt_df_final[opt_df_final['optimized_airhdtat'] == opt_df_final['original_airhdtat']]\n",
    "    \n",
    "    for _, row in unchanged_lanes_data.iterrows():\n",
    "        for status in status_categories:\n",
    "            baseline_col = f'baseline_{status.replace(\" \", \"_\")}%'\n",
    "            optimized_col = f'optimized_{status.replace(\" \", \"_\")}%'\n",
    "            \n",
    "            verification_data.append({\n",
    "                'lane_id': row['lane_id'],\n",
    "                'status_metric': status,\n",
    "                'baseline_%': row[baseline_col],\n",
    "                'optimized_%': row[optimized_col],\n",
    "                'difference': row[optimized_col] - row[baseline_col],\n",
    "                'abs_difference': abs(row[optimized_col] - row[baseline_col])\n",
    "            })\n",
    "    \n",
    "    if verification_data:\n",
    "        verification_df = pd.DataFrame(verification_data)\n",
    "        verification_df.to_excel(writer, sheet_name='Baseline_Verification', index=False)\n",
    "\n",
    "print(\"âœ… Comprehensive results saved!\")\n",
    "print(f\"   ğŸ“Š Main optimization results: Sheet 1\")\n",
    "print(f\"   ğŸ“Š Parameter sensitivity analysis: Sheet 2\") \n",
    "print(f\"   ğŸ“Š Summary metrics comparison: Sheet 3\")\n",
    "print(f\"   ğŸ“Š Baseline preservation verification: Sheet 4\")\n",
    "\n",
    "# ============ FINAL RECOMMENDATIONS ============\n",
    "print(f\"\\nğŸ¯ OPTIMIZATION RECOMMENDATIONS & INSIGHTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate changes\n",
    "changes = {}\n",
    "for status in status_categories:\n",
    "    changes[status] = final_percentages[status] - baseline_percentages[status]\n",
    "\n",
    "# Find biggest improvements and concerns\n",
    "biggest_improvement = max(changes.items(), key=lambda x: x[1])\n",
    "biggest_concern = min(changes.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"ğŸ† BIGGEST IMPROVEMENT: {biggest_improvement[0]} (+{biggest_improvement[1]:.2f}%)\")\n",
    "print(f\"âš ï¸  BIGGEST CONCERN: {biggest_concern[0]} ({biggest_concern[1]:+.2f}%)\")\n",
    "\n",
    "# TAT change analysis\n",
    "tat_changes = opt_df_final[opt_df_final['original_airhdtat'] != opt_df_final['optimized_airhdtat']]\n",
    "total_lanes_changed = len(tat_changes)\n",
    "total_lanes = len(opt_df_final)\n",
    "\n",
    "print(f\"\\nğŸ“Š TAT CHANGE SUMMARY:\")\n",
    "print(f\"   ğŸ“Š Lanes with TAT changes: {total_lanes_changed:,} out of {total_lanes:,} ({total_lanes_changed/total_lanes*100:.1f}%)\")\n",
    "print(f\"   ğŸ“Š Lanes unchanged: {total_lanes - total_lanes_changed:,} ({(total_lanes - total_lanes_changed)/total_lanes*100:.1f}%)\")\n",
    "\n",
    "if not tat_changes.empty:\n",
    "    avg_tat_change = (tat_changes['optimized_airhdtat'] - tat_changes['original_airhdtat']).mean()\n",
    "    print(f\"   ğŸ“Š Average TAT change: {avg_tat_change:+.2f} days\")\n",
    "    \n",
    "    # TAT increase/decrease breakdown\n",
    "    increases = tat_changes[tat_changes['optimized_airhdtat'] > tat_changes['original_airhdtat']]\n",
    "    decreases = tat_changes[tat_changes['optimized_airhdtat'] < tat_changes['original_airhdtat']]\n",
    "    \n",
    "    if len(increases) > 0:\n",
    "        print(f\"   ğŸ“ˆ TAT increases: {len(increases):,} lanes (avg: +{(increases['optimized_airhdtat'] - increases['original_airhdtat']).mean():.2f} days)\")\n",
    "    if len(decreases) > 0:\n",
    "        print(f\"   ğŸ“‰ TAT decreases: {len(decreases):,} lanes (avg: {(decreases['optimized_airhdtat'] - decreases['original_airhdtat']).mean():.2f} days)\")\n",
    "\n",
    "# Target achievement analysis\n",
    "print(f\"\\nğŸ¯ TARGET ACHIEVEMENT ANALYSIS:\")\n",
    "for status in status_categories:\n",
    "    target = target_percentages[status]\n",
    "    achieved = final_percentages[status]\n",
    "    gap = achieved - target\n",
    "    gap_pct = abs(gap) / target * 100 if target != 0 else 0\n",
    "    \n",
    "    if abs(gap) <= tolerance:\n",
    "        status_icon = \"âœ…\"\n",
    "    elif gap_pct <= 10:\n",
    "        status_icon = \"ğŸŸ¡\"\n",
    "    else:\n",
    "        status_icon = \"ğŸ”´\"\n",
    "    \n",
    "    print(f\"   {status_icon} {status}: {achieved:.2f}% (Target: {target:.1f}%, Gap: {gap:+.2f}%)\")\n",
    "\n",
    "# Business impact estimation\n",
    "print(f\"\\nğŸ’¼ ESTIMATED BUSINESS IMPACT:\")\n",
    "total_volume = opt_df_final['volume'].sum()\n",
    "print(f\"   ğŸ“Š Total order volume analyzed: {total_volume:,}\")\n",
    "\n",
    "# Calculate volume impact for each metric\n",
    "for status in status_categories:\n",
    "    volume_change = (final_percentages[status] - baseline_percentages[status]) / 100 * total_volume\n",
    "    print(f\"   ğŸ“Š {status} volume change: {volume_change:+,.0f} orders\")\n",
    "\n",
    "# Parameter sensitivity insights\n",
    "print(f\"\\nğŸ”¬ PARAMETER SENSITIVITY INSIGHTS:\")\n",
    "print(\"   ğŸ’¡ Key findings from sensitivity analysis:\")\n",
    "\n",
    "# Find most sensitive parameters\n",
    "sensitivity_ranges = {}\n",
    "for param in ['score_weight_1day_bp', 'score_weight_2day_bp', 'score_penalty_breach', 'score_penalty_on_promise']:\n",
    "    param_data = sensitivity_df[param]\n",
    "    breach_range = sensitivity_df['Breach'].max() - sensitivity_df['Breach'].min()\n",
    "    sensitivity_ranges[param] = breach_range\n",
    "\n",
    "most_sensitive = max(sensitivity_ranges.items(), key=lambda x: x[1])\n",
    "least_sensitive = min(sensitivity_ranges.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"   ğŸ“Š Most sensitive parameter: {most_sensitive[0]} (Breach range: {most_sensitive[1]:.2f}%)\")\n",
    "print(f\"   ğŸ“Š Least sensitive parameter: {least_sensitive[0]} (Breach range: {least_sensitive[1]:.2f}%)\")\n",
    "\n",
    "# Baseline preservation summary\n",
    "print(f\"\\nğŸ”’ BASELINE PRESERVATION SUMMARY:\")\n",
    "unchanged_count = len(opt_df_final[opt_df_final['optimized_airhdtat'] == opt_df_final['original_airhdtat']])\n",
    "print(f\"   ğŸ“Š Lanes preserving original TAT: {unchanged_count:,} out of {len(opt_df_final):,}\")\n",
    "print(f\"   âœ… These lanes maintain EXACT baseline percentages\")\n",
    "print(f\"   ğŸ“Š Preservation rate: {unchanged_count/len(opt_df_final)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ACTIONABLE RECOMMENDATIONS:\")\n",
    "print(\"   1. ğŸ“ˆ Focus on lanes with highest volume impact for manual review\")\n",
    "print(\"   2. ğŸ” Monitor breach percentage closely - current optimized level is balanced\")\n",
    "print(\"   3. âš™ï¸  Consider adjusting parameters based on sensitivity analysis if targets not met\")\n",
    "print(\"   4. ğŸ“Š Implement gradual rollout starting with lanes showing biggest improvements\")\n",
    "print(\"   5. ğŸ”„ Re-run optimization monthly with fresh data for continuous improvement\")\n",
    "print(\"   6. âœ… Validate that unchanged lanes preserve baseline metrics exactly\")\n",
    "print(\"   7. ğŸ¯ Use top sensitivity combinations for fine-tuning toward specific targets\")\n",
    "\n",
    "# Final execution summary\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nğŸ‰ OPTIMIZATION PROCESS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"â±ï¸  Total execution time: {execution_time:.2f} seconds\")\n",
    "print(f\"ğŸ“ Main results file: {output_filename}\")\n",
    "print(f\"ğŸ“ Sensitivity analysis table: {sensitivity_table_filename}\")\n",
    "print(f\"ğŸ¯ Target achievement status: {'âœ… Mostly Achieved' if sum(abs(final_percentages[s] - target_percentages[s]) <= tolerance for s in status_categories) >= 3 else 'ğŸŸ¡ Partially Achieved'}\")\n",
    "print(f\"ğŸ”’ Baseline preservation: {'âœ… Verified' if unchanged_count > 0 else 'â„¹ï¸ No unchanged lanes'}\")\n",
    "\n",
    "print(f\"\\nâœ¨ Thank you for using the Enhanced TAT Optimization Script! âœ¨\")\n",
    "print(\"ğŸ“§ For questions or improvements, please refer to the comprehensive output files.\")\n",
    "print(\"ğŸ”„ Remember to validate results with business stakeholders before implementation.\")\n",
    "print(\"\\nğŸ”§ KEY IMPROVEMENTS IN THIS VERSION:\")\n",
    "print(\"   âœ… Exact baseline preservation for unchanged TAT values\")\n",
    "print(\"   âœ… Enhanced verification and reporting\")\n",
    "print(\"   âœ… Improved fallback constraint handling\")\n",
    "print(\"   âœ… Comprehensive baseline preservation verification sheet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b05bf-19fd-4669-b93b-5d57824534b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
